# Day 2 - Supervised Learning: k-NN & Model Complexity (Chapter 2: Pages 25-40)

## ðŸ“– Topics Covered:
- Deep dive into **supervised learning** concepts.
- Expanded on **k-Nearest Neighbors (k-NN)** classifier behavior.
- Explored **model complexity** and **generalization**:
  - **Underfitting** vs. **Overfitting** trade-off.
- Visualized how **decision boundaries** change with different model complexities.
- Introduction to **multiclass classification**:
  - Used **One-vs-Rest (OvR)** strategy to extend k-NN for multiple classes.

## ðŸ§  Key Learnings:
- Adjusting **k** in k-NN controls **model complexity**:
  - Low k â†’ complex boundaries â†’ potential overfitting.
  - High k â†’ smoother boundaries â†’ risk of underfitting.
- **Generalization** ensures models perform well on **unseen data**.
- Learned about **multiclass strategies** like **One-vs-Rest (OvR)**.

## ðŸš€ Reflections:
- Visualizations made the concepts of **overfitting vs. underfitting** very clear.
- Adjusting model complexity helped understand **bias-variance trade-offs**.

## ðŸ”— Related Files:
- **Notebook**: [02-knn-breast-cancer.ipynb](../JupyterNotebooks/02-knn-breast-cancer.ipynb)
- **Notebook**: [02-knn-forge.ipynb](../JupyterNotebooks/02-knn-forge.ipynb)